\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subcaption}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{SCITEPRESS}     % Please add other packages that you may need BEFORE the SCITEPRESS.sty package.


\begin{document}

\title{A Case Study on Performance Optimization Techniques in Java Programming}

\author{\authorname{Ciprian Khlud\sup{1}\orcidAuthor{0000-0001-6211-3199}
  and Cristian Fr\u asinaru\sup{1}\orcidAuthor{0000-0002-5246-7396} }
\affiliation{\sup{1}"Alexandru Ioan Cuza University", Ia\c si, Romania}
\email{ciprian.mustiata@gmail.com, acf@info.uaic.ro}
}

\keywords{Java, Runtime performance, Memory usage, Garbage collection, Sequence analysis, SAM/BAM files}

\abstract{
Choosing the right programming platform for processor or memory intensive applications is a subject that is debated in all types of contexts.
When analyzing the performance of a specific platform, equally important is the usage of appropriate language specific constructions and 
programming interfaces (APIs).
In this paper we investigate how a state-of-the art implementation, part of a multi-threaded framework for sequence analysis (elPrep)
could benefit from various optimization techniques dedicated to improving the runtime performance of Java applications.
ElPrep is an established tool for processing SAM and BAM files in sequencing pipelines. 
In order to migrate from its original implementation to a different software platform, more suitable for memory intensive tasks, 
the authors have re-implemented elPrep in Java, Go and C++ and benchmarked their performance.
Overall, the Go implementation won by a good margin, considering a metric that involved both the RAM usage and the runtime performance.
We show that, without changing the semantics of the algorithm, by using appropriate programming techniques we are able to significantly improve
the behavior of the Java implementation, to a point that may even alter the conclusions of the original study.
}

\onecolumn \maketitle \normalsize \setcounter{footnote}{0} \vfill

\section{\uppercase{Introduction}}
\label{sec:introduction}

In the field of bioinformatics, DNA sequence analysis generally consists of processing large amounts of data and performing various operations on it, 
such as sequence alignment, variant detection, searches against biological databases, etc.
A large variety of software tools exists for these operations, most of them having specific uses cases but with a common denominator 
regarding the fact they need to perform processor and memory intensive tasks: 
I/O operations on large file, compression/decompression, text processing, etc. \cite{?}

Choosing a programming platform that offers all the required instruments to handle the specific challenges in bioinformatics is important, 
as pointed out in a recent study dedicated to migrating an existing Common Lisp application, called elPrep, 
to another platform with better support for memory management and concurrency \cite{costanza:2019}.
ElPrep \cite{herzeel:2019} is a a multithreaded tool for preparing sequence alignment/map files (SAM/BAM)
for variant calling in DNA sequencing pipelines. 
A key feature of elPrep is the ability to avoid the standard practice of creating a pipeline consisting of multiple command line tools invocations, 
executing a single pass through a SAM/BAM file and keeping data as much as possible in main memory.
%As the software executes concurrently different steps of the algorithm, it needs to process large amounts of data (hundreds of gigabytes) in an efficient way.
In \cite{costanza:2019} the authors investigated Go, Java and C++ programming platforms, as an alternative to Common Lisp.
%Go and Java using a concurrent, parallel garbage collector on the one hand, and C++17 using reference counting on the other hand for handling
Te result of their study concluded that the Go implementation performed best, using a metric that involved both the RAM usage and the runtime performance.
The benchmarks of the study showed that Java had a faster runtime, but a significantly higher memory usage, while Go offered a better balance between the two.

As the Java source code for elPrep is available at {\tt https://github.com/exascience/elprep-bench}, we have analyzed key aspects regarding the memory management and
thread synchronization, and propose a series of improvements that could increase significantly the performance of the Java implementation. 
%benefit other studies of this kind.


\section{\uppercase{Background}}

\subsection{\uppercase{Garbage Collection}}

In order to analyze the behavior of memory intensive applications,
%especially those based on frequent allocation and deallocation, 
it is important to understand how garbage collection works and especially how Java \cite{java} implements its garbage collectors.

The Java Virtual Machine (JVM) \cite{lindholm:2014} offers an automatic storage management system, called {\it garbage collector (GC)} which reclaims heap storage occupied by objects which are no longer used.
The garbage collection process \cite{gc:oracle} works typically by splitting the heap into two regions: a {\it young generation} region and an {\it old generation}.
All new objects are allocated in the young region, in a very fast manner, using typically a "bump-pointer" strategy. When this region becomes full  a {\it minor} garbage collection occurs and all dead objects are deleted very quickly. The objects which are still referenced survive and they  are moved to the old generation. This minor collection is always a "stop the world" event, meaning that all of the application threads will be paused until the GC is finished. 
In the old generation, objects are expected to live longer and they are collected more seldom but with a more expensive algorithm, called {\it major} garbage collection.

The algorithm used by GC has two steps. The first one is to {\it mark} the objects that are still used from the heap. In the second step, it {\it sweeps (deletes)} the objects which have not been marked (dead), leaving only referenced objects and pointers to free space. 
%To further improve performance, in addition to deleting unreferenced objects, you can also compact the remaining referenced objects. By moving referenced object together, this makes new memory allocation much easier and faster.
Therefore, the speed of GC depends on two factors: the number of objects it has to analyze and the complexity of the relationships between them. %collecting graph

Considering the behavior we have described so far, we will analyze the impact of some simple tweaks
meant to reduce the impact of GC over the application performance, such as:
reducing the unnecessary small allocations in young region, controlling the scope in which objects are referenced in order to minimize the number of times when expensive collection of old region is triggered, simplifying the object graph and controlling the amount of memory JVM is allowed to use.


\subsection{\uppercase{Threads, Locks and the File System}}

Java platform supports concurrent programming by using threads \cite{gosling:2014}.
Multiple threads can execute at the same time, taking advantage of computing units that have more than one processor and of processors that have more than one core. Threads performing operations that are not atomic will interleave when they access shared data.
A {\it synchronized} statement acquires a mutual-exclusion lock when entering a critical section, executes the block that references the shared data, then releases the lock.
While one thread owns the lock for that data, no other thread may access it. 
The proper use of this mechanism is crucial for the concurrent implementation of an algorithm. If threads are waiting to much on locked resources, the overall performance of the application will suffer.

In our case study, multiple threads are performing operations on the file system, reading and writing large amounts of strings from and into text files.
Regardless of the operating system and programming language, the underlying hardware is optimized to work with streams of bytes. In an atomic operation, data is read into a buffer of bytes, in a contiguous manner. Similarly, data is written into a buffer of bytes that is flushed afterwards to the file. Both these operations are single threaded by design and historical reasons, so Java libraries have locks to make the access to streams single-threaded. A \texttt{write} method usually looks like this:
\begin{small}
\begin{verbatim}
public void write(String s, int off, int len){
  synchronized (lock) { ... }
}
\end{verbatim}
\end{small}

%The algorithm for writing to disk is the following:
%
%\begin{small}
%	\begin{verbatim}
%	allSamAlignments.map(
%	  samAlignment -> formatRow(samAlignment);
%	)
%	.forEach(
%	  formattedRow -> writeToBuffer(formattedRow));
%	\end{verbatim}
%\end{small}

We will show that creating a large number of short lived strings and writing them to a file in a multithreaded manner will generate a behavior similar to using a single thread, plus the overhead of acquiring and releasing the synchronization lock.


%So what we did, we rewrote the code to be single threaded, so we don't call any OS to lock and release a mutex and the pseudo-code would look like following:
%
%\begin{small}
%	\begin{verbatim}
%allSamAlignments.forEach(samAlignment ->
%{
%  writeToBufferFormatted(samAlignment);
%});
%	\end{verbatim}
%\end{small}

%The main take away is that the updated code can write directly to underlying buffer and it can be made quite fast and it will need basically no allocation, as it will be shown.


\section{\uppercase{Memory Usage}}

The Java Virtual Machine allocates memory either on stack or on heap \cite{lindholm:2014}. 
The {\it heap} is the place where all class instances and arrays are allocated and it is shared among all threads. Each JVM thread has a private {\it stack} which holds local variables and partial results  during successive method invocations and returns. 
%The JVM might do escape analysis and decide to allocate objects directly on the stack, which means cleaning them up when the method ends is free.
When working with large amounts of objects, it is quite important to assess the memory consumption of a data structure, in similar way as the {\it sizeof} construct in C or C++.
%JVMs may move some heap allocations to stack allocations, but this would not matter if we allocate long running objects and as we talked in section about GC, the GC will have also some pre-allocated memory to do future expected allocations.
An object allocated on the heap has a {\it header} which contains information used for locking, garbage collection or the identity of that object. The size of the header depends on the operating system, and it may be 8 bytes on 32 bit architectures or 16 bytes on 64 bit architectures. 
Also, for performance reasons and in order to conform with most of the hardware architectures, JVM will {\it align} data.
That means that if we have an object that wraps just one byte, it will not use: $8 \text{(object header)} + 1 \text{(content)} = 9 $ bytes of memory on the heap, but it will use $16$ bytes as it needs to be aligned to the next $8$ byte boundary.
%A technical brief description made by a JVM engineer describes why and how was it done: https://shipilev.net/jvm/anatomy-quarks/24-object-alignment/

In Java, strings are objects and they are allocated on the heap. That fact that string literals are stored in a shared object pool, in order to reduce memory consumption, is of no relevance in our context.
Inspecting the source code of the \texttt{String} class, one can observe the following instance fields:
\begin{small}
\begin{verbatim}
private final byte[] value;
private final byte coder;
private int hash; // Default to 0
\end{verbatim}
\end{small}

As expected, a \texttt{String} object keeps a reference to an internal byte array. However, the other two fields will make the size of the object equal to: $8$ (header) $+$ $4$ \texttt{value} reference $+$ $1$  \texttt{coder} value $+$ $4$ \texttt{hash} value $= 17$ bytes. Being aligned to $8$ bytes, it will actually use $24$ bytes.
When creating many \texttt{String} instances (like millions of them, as in our case study), the extra information included in this class will add up, consuming memory and  triggering the garbage collector more often then necessary. 

We will show that replacing the \texttt{String} usage to the underlying \texttt{value} byte array will improve the performance of the application, and this approach should be implemented in every scenario that involves processing large amounts of text data.
% especially if it is done at million times scale, will be translated both in fewer GCs (as are saved few tenths of MB), fewer 24 bytes * million instances bytes to copy and visit for marking. 


\bibliographystyle{apalike}
{\small
\bibliography{elprep-study}}


\end{document}

